{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ac25ea",
   "metadata": {},
   "source": [
    "# Fine-Tuning DistilBERT\n",
    "The goal is to Fine-tune DistilBERT to predict sentiment on the Twitter dataset.\n",
    "\n",
    "## About Dataset\n",
    "#### Context\n",
    "This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment .\n",
    "\n",
    "#### Content\n",
    "It contains the following 6 fields:\n",
    "\n",
    "target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "\n",
    "ids: The id of the tweet ( 2087)\n",
    "\n",
    "date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "\n",
    "flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "\n",
    "user: the user that tweeted (robotickilldozr)\n",
    "\n",
    "text: the text of the tweet (Lyx is cool)\n",
    "\n",
    "#### Acknowledgements\n",
    "The official link regarding the dataset with resources about how it was generated is here\n",
    "The official paper detailing the approach is here\n",
    "\n",
    "#### Citation: \n",
    "Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "739d6968-9486-4b85-ae4d-700920567e8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex Chung\\Documents\\the_Lab\\Portfolio\\ml_engineering\\fresh_venv_new\\lib\\site-packages\\transformers\\utils\\generic.py:441: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\Alex Chung\\Documents\\the_Lab\\Portfolio\\ml_engineering\\fresh_venv_new\\lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n",
      "C:\\Users\\Alex Chung\\Documents\\the_Lab\\Portfolio\\ml_engineering\\fresh_venv_new\\lib\\site-packages\\transformers\\utils\\generic.py:309: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  _torch_pytree._register_pytree_node(\n"
     ]
    }
   ],
   "source": [
    "# Importing library\n",
    "from datasets import Dataset, Features, ClassLabel, Value\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf38cfe7-9111-4bfe-b8f6-1a9a636e0cf2",
   "metadata": {},
   "source": [
    "## 1. Loading and Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0ab42732-0fa5-495c-8273-b389024fc57a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and preprocess data\n",
    "path = \"c:\\\\Users\\\\Alex Chung\\\\Documents\\\\the_Lab\\\\Portfolio\\\\ml_engineering\\\\data\\\\sentiment140\\\\\"\n",
    "file = \"training.1600000.processed.noemoticon.csv\"\n",
    "df = pd.read_csv(path + file, \n",
    "                 encoding=\"ISO-8859-1\", names=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aae2320d-ed66-4d55-9d44-d20b0d901401",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>id</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   target          id                          date      flag  \\\n",
       "0       0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1       0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2       0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3       0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4       0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "\n",
       "              user                                               text  \n",
       "0  _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1    scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2         mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3          ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4           Karoli  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4135177d-8abf-4175-8ca7-957baa78f06d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 6 columns):\n",
      " #   Column  Non-Null Count    Dtype \n",
      "---  ------  --------------    ----- \n",
      " 0   target  1600000 non-null  int64 \n",
      " 1   id      1600000 non-null  int64 \n",
      " 2   date    1600000 non-null  object\n",
      " 3   flag    1600000 non-null  object\n",
      " 4   user    1600000 non-null  object\n",
      " 5   text    1600000 non-null  object\n",
      "dtypes: int64(2), object(4)\n",
      "memory usage: 73.2+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295d4835-b8c8-4c3c-b57a-3bc77b4882c1",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "90953f4d-8bc4-4ac6-9882-f390a3185731",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Label Distribution:\n",
      "target\n",
      "0    0.5004\n",
      "1    0.4996\n",
      "Name: proportion, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Taking only 10,000 samples\n",
    "df = df[[\"target\", \"text\"]].sample(10000, random_state=42)\n",
    "df[\"target\"] = df[\"target\"].map({0: 0, 4: 1})\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Define dataset features\n",
    "features = Features({\n",
    "    \"target\": ClassLabel(names=[\"negative\", \"positive\"]),\n",
    "    \"text\": Value(dtype=\"string\")\n",
    "})\n",
    "dataset = Dataset.from_pandas(df, features=features)\n",
    "\n",
    "# Check original distribution\n",
    "print(\"Original Label Distribution:\")\n",
    "print(df[\"target\"].value_counts(normalize=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a9b5411-b48c-4052-8c77-724958c8facb",
   "metadata": {},
   "source": [
    "### Tokenize Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e644d00-55d9-47b5-980c-b80df28fb73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex Chung\\Documents\\the_Lab\\Portfolio\\ml_engineering\\fresh_venv_new\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "04046baaa2fe4fcfbfbd589f8e83229d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset columns: ['labels', 'input_ids', 'attention_mask']\n"
     ]
    }
   ],
   "source": [
    "# Tokenize, preserving labels\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Rename target to labels\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"target\", \"labels\")\n",
    "\n",
    "# Verify dataset columns\n",
    "print(\"Tokenized dataset columns:\", tokenized_dataset.column_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6088edcf-fd0e-4e56-b80d-0a50b3454dcf",
   "metadata": {},
   "source": [
    "### Spliting into train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b945528-ee75-4d3a-9dde-0fd9d0ca8725",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Train size: 8000, Test size: 2000\n",
      "Train dataset columns: ['labels', 'input_ids', 'attention_mask']\n",
      "Test dataset columns: ['labels', 'input_ids', 'attention_mask']\n",
      "Train Label Distribution:\n",
      "0    0.500375\n",
      "1    0.499625\n",
      "Name: proportion, dtype: float64\n",
      "Test Label Distribution:\n",
      "0    0.5005\n",
      "1    0.4995\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "First Train Example:\n",
      "{'labels': 1, 'input_ids': [101, 2074, 2513, 2013, 7873, 2777, 1012, 2986, 2396, 2265, 1010, 13366, 16294, 4221, 2135, 1037, 3459, 2846, 1997, 2147, 1010, 4169, 1999, 3327, 2001, 6581, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "# Stratified split\n",
    "train_test = tokenized_dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column=\"labels\")\n",
    "train_dataset = train_test[\"train\"]\n",
    "test_dataset = train_test[\"test\"]\n",
    "\n",
    "# Verify split and columns\n",
    "print(f\"\\nTrain size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n",
    "print(\"Train dataset columns:\", train_dataset.column_names)\n",
    "print(\"Test dataset columns:\", test_dataset.column_names)\n",
    "train_dist = pd.Series(train_dataset[\"labels\"]).value_counts(normalize=True)\n",
    "test_dist = pd.Series(test_dataset[\"labels\"]).value_counts(normalize=True)\n",
    "print(\"Train Label Distribution:\")\n",
    "print(train_dist)\n",
    "print(\"Test Label Distribution:\")\n",
    "print(test_dist)\n",
    "\n",
    "# Inspect tokenized dataset\n",
    "print(\"\\nFirst Train Example:\")\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c14c7101-42f5-4043-a910-7ff76bdf4ede",
   "metadata": {},
   "source": [
    "## 3. Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ecc65c14-1259-47ad-9cdb-0d47bfb742cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf9358f1-ba2e-47eb-a4cd-0398bedcc85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up Trainer\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"C:/Users/Alex Chung/Documents/ml_engineering_clean/results\",\n",
    "    logging_dir=\"C:/Users/Alex Chung/Documents/ml_engineering_clean/logs\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",  # Changed from eval_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda eval_pred: {\n",
    "        \"accuracy\": (np.argmax(eval_pred.predictions, axis=1) == eval_pred.label_ids).mean()\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12ad5971-3fb6-4b3e-a2b9-abe060bf1189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Label Distribution:\n",
      "target\n",
      "0    0.5004\n",
      "1    0.4996\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex Chung\\Documents\\the_Lab\\Portfolio\\ml_engineering\\fresh_venv_new\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10eef10aa9054f9ca7410739f4cc5e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset columns: ['labels', 'input_ids', 'attention_mask']\n",
      "\n",
      "Train size: 8000, Test size: 2000\n",
      "Train dataset columns: ['labels', 'input_ids', 'attention_mask']\n",
      "Test dataset columns: ['labels', 'input_ids', 'attention_mask']\n",
      "Train Label Distribution:\n",
      "0    0.500375\n",
      "1    0.499625\n",
      "Name: proportion, dtype: float64\n",
      "Test Label Distribution:\n",
      "0    0.5005\n",
      "1    0.4995\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "First Train Example:\n",
      "{'labels': 1, 'input_ids': [101, 2074, 2513, 2013, 7873, 2777, 1012, 2986, 2396, 2265, 1010, 13366, 16294, 4221, 2135, 1037, 3459, 2846, 1997, 2147, 1010, 4169, 1999, 3327, 2001, 6581, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 1:17:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.426900</td>\n",
       "      <td>0.444462</td>\n",
       "      <td>0.793000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.286700</td>\n",
       "      <td>0.489853</td>\n",
       "      <td>0.807500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.739199</td>\n",
       "      <td>0.802500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.30671591504414875, metrics={'train_runtime': 4676.5072, 'train_samples_per_second': 5.132, 'train_steps_per_second': 0.321, 'total_flos': 267277814425728.0, 'train_loss': 0.30671591504414875, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4675c3a6",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31d5e5ee-de5f-4e66-9466-e37e04c5dccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.4444619417190552, 'eval_accuracy': 0.793, 'eval_runtime': 114.8718, 'eval_samples_per_second': 17.411, 'eval_steps_per_second': 1.088, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "trainer.save_model(\"C:/Users/Alex Chung/Documents/ml_engineering_clean/final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "947de72a-1e18-4e1e-917c-a90444653fcf",
   "metadata": {},
   "source": [
    "### Observations\n",
    "The model indicates there's overfitting because training scores improve with each epoch, yet validation scores get worse. \n",
    "\n",
    "There are a few things try to improve the model:\n",
    "- Add regularization\n",
    "- Use Early Stopping if validation doesn't improve over 1 epoch\n",
    "- Adjust Learning Rate smaller\n",
    "- Increase Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "df90609a-9113-4fd4-93ae-1a10792b9df3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import EarlyStoppingCallback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "658e554f-f743-44a5-ab64-9d47668a2bcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1000' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1000/1500 48:24 < 24:15, 0.34 it/s, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.427600</td>\n",
       "      <td>0.423437</td>\n",
       "      <td>0.805500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.326900</td>\n",
       "      <td>0.466076</td>\n",
       "      <td>0.799000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1000, training_loss=0.4022690372467041, metrics={'train_runtime': 2908.4469, 'train_samples_per_second': 8.252, 'train_steps_per_second': 0.516, 'total_flos': 178127255130240.0, 'train_loss': 0.4022690372467041, 'epoch': 2.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"C:/Users/Alex Chung/Documents/ml_engineering_clean/results\",\n",
    "    logging_dir=\"C:/Users/Alex Chung/Documents/ml_engineering_clean/logs\",\n",
    "    num_train_epochs=3,  # Kept at 3 but added early stopping if doesn't improve in 1 epoch\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_accuracy\",  # Optimize for accuracy\n",
    "    greater_is_better=True,\n",
    "    logging_steps=100,\n",
    "    weight_decay=0.01,  # Add regularization\n",
    "    learning_rate=2e-5,  # Lower learning rate\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda eval_pred: {\n",
    "        \"accuracy\": (np.argmax(eval_pred.predictions, axis=1) == eval_pred.label_ids).mean()\n",
    "    },\n",
    "    callbacks=[EarlyStoppingCallback(early_stopping_patience=1)] # early stopping\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5de273ee-0817-41a4-a8be-ff44638e971b",
   "metadata": {},
   "source": [
    "### Observations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9788f2af-2d0e-4ec7-bac8-eb63613eda96",
   "metadata": {},
   "source": [
    "- Final eval accuracy: 0.8055 (meets >80% requirement).\n",
    "- Training/validation loss and accuracy per epoch (as shown above).\n",
    "- Steps taken to address overfitting (early stopping, weight decay, lower learning rate).\n",
    "- Challenges faced (e.g., numpy errors, cl.exe, labels, evaluation_strategy) and resolutions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "413ab18d-fca5-41a1-8c07-685f18939e3a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresh_venv_new (Python 3.10)",
   "language": "python",
   "name": "fresh_venv_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
