{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f0ac25ea",
   "metadata": {},
   "source": [
    "# Fine-Tuning DistilBERT\n",
    "The goal is to Fine-tune DistilBERT to predict sentiment on the Twitter dataset.\n",
    "\n",
    "## About Dataset\n",
    "#### Context\n",
    "This is the sentiment140 dataset. It contains 1,600,000 tweets extracted using the twitter api . The tweets have been annotated (0 = negative, 4 = positive) and they can be used to detect sentiment .\n",
    "\n",
    "#### Content\n",
    "It contains the following 6 fields:\n",
    "\n",
    "target: the polarity of the tweet (0 = negative, 2 = neutral, 4 = positive)\n",
    "\n",
    "ids: The id of the tweet ( 2087)\n",
    "\n",
    "date: the date of the tweet (Sat May 16 23:58:44 UTC 2009)\n",
    "\n",
    "flag: The query (lyx). If there is no query, then this value is NO_QUERY.\n",
    "\n",
    "user: the user that tweeted (robotickilldozr)\n",
    "\n",
    "text: the text of the tweet (Lyx is cool)\n",
    "\n",
    "#### Acknowledgements\n",
    "The official link regarding the dataset with resources about how it was generated is here\n",
    "The official paper detailing the approach is here\n",
    "\n",
    "#### Citation: \n",
    "Go, A., Bhayani, R. and Huang, L., 2009. Twitter sentiment classification using distant supervision. CS224N Project Report, Stanford, 1(2009), p.12.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c3916c9-fa6d-47b4-8e98-74e0ae590738",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset columns: ['labels', 'input_ids', 'attention_mask']\n",
      "Test dataset columns: ['labels', 'input_ids', 'attention_mask']\n",
      "First train example: {'labels': 1, 'input_ids': [101, 2074, 2513, 2013, 7873, 2777, 1012, 2986, 2396, 2265, 1010, 13366, 16294, 4221, 2135, 1037, 3459, 2846, 1997, 2147, 1010, 4169, 1999, 3327, 2001, 6581, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    }
   ],
   "source": [
    "print(\"Train dataset columns:\", train_dataset.column_names)\n",
    "print(\"Test dataset columns:\", test_dataset.column_names)\n",
    "print(\"First train example:\", train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "12ad5971-3fb6-4b3e-a2b9-abe060bf1189",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Label Distribution:\n",
      "target\n",
      "0    0.5004\n",
      "1    0.4996\n",
      "Name: proportion, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Alex Chung\\Documents\\the_Lab\\Portfolio\\ml_engineering\\fresh_venv_new\\lib\\site-packages\\huggingface_hub\\file_download.py:896: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10eef10aa9054f9ca7410739f4cc5e24",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset columns: ['labels', 'input_ids', 'attention_mask']\n",
      "\n",
      "Train size: 8000, Test size: 2000\n",
      "Train dataset columns: ['labels', 'input_ids', 'attention_mask']\n",
      "Test dataset columns: ['labels', 'input_ids', 'attention_mask']\n",
      "Train Label Distribution:\n",
      "0    0.500375\n",
      "1    0.499625\n",
      "Name: proportion, dtype: float64\n",
      "Test Label Distribution:\n",
      "0    0.5005\n",
      "1    0.4995\n",
      "Name: proportion, dtype: float64\n",
      "\n",
      "First Train Example:\n",
      "{'labels': 1, 'input_ids': [101, 2074, 2513, 2013, 7873, 2777, 1012, 2986, 2396, 2265, 1010, 13366, 16294, 4221, 2135, 1037, 3459, 2846, 1997, 2147, 1010, 4169, 1999, 3327, 2001, 6581, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.bias', 'classifier.weight', 'pre_classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1500' max='1500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1500/1500 1:17:51, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.426900</td>\n",
       "      <td>0.444462</td>\n",
       "      <td>0.793000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.286700</td>\n",
       "      <td>0.489853</td>\n",
       "      <td>0.807500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.129000</td>\n",
       "      <td>0.739199</td>\n",
       "      <td>0.802500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1500, training_loss=0.30671591504414875, metrics={'train_runtime': 4676.5072, 'train_samples_per_second': 5.132, 'train_steps_per_second': 0.321, 'total_flos': 267277814425728.0, 'train_loss': 0.30671591504414875, 'epoch': 3.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, Features, ClassLabel, Value\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments, DataCollatorWithPadding\n",
    "\n",
    "# Load and preprocess data\n",
    "path = \"c:\\\\Users\\\\Alex Chung\\\\Documents\\\\the_Lab\\\\Portfolio\\\\ml_engineering\\\\data\\\\sentiment140\\\\\"\n",
    "file = \"training.1600000.processed.noemoticon.csv\"\n",
    "df = pd.read_csv(path + file, \n",
    "                 encoding=\"ISO-8859-1\", names=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"])\n",
    "df = df[[\"target\", \"text\"]].sample(10000, random_state=42)\n",
    "df[\"target\"] = df[\"target\"].map({0: 0, 4: 1})\n",
    "df = df.reset_index(drop=True)\n",
    "\n",
    "# Define dataset features\n",
    "features = Features({\n",
    "    \"target\": ClassLabel(names=[\"negative\", \"positive\"]),\n",
    "    \"text\": Value(dtype=\"string\")\n",
    "})\n",
    "dataset = Dataset.from_pandas(df, features=features)\n",
    "\n",
    "# Check original distribution\n",
    "print(\"Original Label Distribution:\")\n",
    "print(df[\"target\"].value_counts(normalize=True))\n",
    "\n",
    "# Tokenize, preserving labels\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], truncation=True, padding=False)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True, remove_columns=[\"text\"])\n",
    "\n",
    "# Rename target to labels\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"target\", \"labels\")\n",
    "\n",
    "# Verify dataset columns\n",
    "print(\"Tokenized dataset columns:\", tokenized_dataset.column_names)\n",
    "\n",
    "# Stratified split\n",
    "train_test = tokenized_dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column=\"labels\")\n",
    "train_dataset = train_test[\"train\"]\n",
    "test_dataset = train_test[\"test\"]\n",
    "\n",
    "# Verify split and columns\n",
    "print(f\"\\nTrain size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n",
    "print(\"Train dataset columns:\", train_dataset.column_names)\n",
    "print(\"Test dataset columns:\", test_dataset.column_names)\n",
    "train_dist = pd.Series(train_dataset[\"labels\"]).value_counts(normalize=True)\n",
    "test_dist = pd.Series(test_dataset[\"labels\"]).value_counts(normalize=True)\n",
    "print(\"Train Label Distribution:\")\n",
    "print(train_dist)\n",
    "print(\"Test Label Distribution:\")\n",
    "print(test_dist)\n",
    "\n",
    "# Inspect tokenized dataset\n",
    "print(\"\\nFirst Train Example:\")\n",
    "print(train_dataset[0])\n",
    "\n",
    "# Set up Trainer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"C:/Users/Alex Chung/Documents/ml_engineering_clean/results\",\n",
    "    logging_dir=\"C:/Users/Alex Chung/Documents/ml_engineering_clean/logs\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",  # Changed from eval_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_steps=100,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=lambda eval_pred: {\n",
    "        \"accuracy\": (np.argmax(eval_pred.predictions, axis=1) == eval_pred.label_ids).mean()\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "31d5e5ee-de5f-4e66-9466-e37e04c5dccb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:53]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Results: {'eval_loss': 0.4444619417190552, 'eval_accuracy': 0.793, 'eval_runtime': 114.8718, 'eval_samples_per_second': 17.411, 'eval_steps_per_second': 1.088, 'epoch': 3.0}\n"
     ]
    }
   ],
   "source": [
    "eval_results = trainer.evaluate()\n",
    "print(\"Evaluation Results:\", eval_results)\n",
    "trainer.save_model(\"C:/Users/Alex Chung/Documents/ml_engineering_clean/final_model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa2e9dbe",
   "metadata": {},
   "source": [
    "## 1. Loading and Inspecting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51724aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = \"c:\\\\Users\\\\Alex Chung\\\\Documents\\\\the_Lab\\\\Portfolio\\\\ml_engineering\\\\data\\\\sentiment140\\\\\"\n",
    "df = pd.read_csv(path+\"training.1600000.processed.noemoticon.csv\", encoding=\"ISO-8859-1\", names=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bde291f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2828ce7f-acf1-4b9f-9df3-334b618f42b9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8976bdc4-1d04-4723-b44f-cda9aa273f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8259063f-5bc1-43cd-ad89-01f0ee504b2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df.text.to_list()[:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0300509",
   "metadata": {},
   "source": [
    "## 2. Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfeea30-5c7d-4656-8842-eb1a804646c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load subset of Twitter data\n",
    "df = df[[\"target\", \"text\"]].sample(10000, random_state=42)  # Subset for speed\n",
    "df[\"target\"] = df[\"target\"].map({0: 0, 4: 1})  # Map labels\n",
    "df = df.reset_index(drop=True)  # Reset index to avoid __index_level_0__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36193a06-0532-4aa7-9387-95380843c72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0196bf71-4b1b-4634-aa58-7a76696fe872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset features with ClassLabel for target\n",
    "features = Features({\n",
    "    \"target\": ClassLabel(names=[\"negative\", \"positive\"]),  # Define 0=negative, 1=positive\n",
    "    \"text\": Value(\"string\")\n",
    "})\n",
    "dataset = Dataset.from_pandas(df, features=features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "056b3f19-47d6-4885-a8d2-19bd55e6787c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d9c327-9746-4be4-aab7-07880723bd3a",
   "metadata": {},
   "source": [
    "### Checking the tokenized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0450b369-b805-47f7-9380-3cf4a9a4e486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Dataset overview\n",
    "print(\"Dataset Info:\")\n",
    "print(tokenized_dataset)\n",
    "print(\"Columns:\", tokenized_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c9195d1-caaa-48eb-966b-d13705c71f9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Single example\n",
    "print(\"First Example:\")\n",
    "print(tokenized_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6662907-157a-4e92-ad1b-31379ee71585",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Multiple examples as table\n",
    "print(\"First 5 Examples:\")\n",
    "df_tokenized = tokenized_dataset.select(range(5)).to_pandas()\n",
    "df_tokenized[['text', 'target', 'input_ids', 'attention_mask']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1b6a6e-cedd-4acb-a1a3-1a9154715c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Decode tokens\n",
    "print(\"Decoded Example:\")\n",
    "sample = tokenized_dataset[0]\n",
    "decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "print(f\"Original: {sample['text']}\")\n",
    "print(f\"Decoded: {decoded_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc7fd4a0-02ae-4fca-98a3-4f6d1b0fb01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Verify lengths\n",
    "lengths = [len(sample['input_ids']) for sample in tokenized_dataset]\n",
    "print(f\"\\nAll lengths 512? {all(length == 512 for length in lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d80b6f8a-5a11-4afa-9314-1f17defffc3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Check labels\n",
    "unique_labels = set(tokenized_dataset['target'])\n",
    "print(f\"Labels: {unique_labels}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94861a1-4fc5-427b-8978-cc400c6535dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Inspect attention mask\n",
    "print(\"Attention Mask Example:\")\n",
    "token_count = sum(sample['attention_mask'])\n",
    "print(f\"Non-padding tokens: {token_count}\")\n",
    "print(f\"First 10 input_ids: {sample['input_ids'][:10]}\")\n",
    "print(f\"First 10 attention_mask: {sample['attention_mask'][:10]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7dbcd8-4bc5-4db8-8c48-33d0814d6a3d",
   "metadata": {},
   "source": [
    "### Splitting the tokenized dataset into stratefied train test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3bb02ea-2673-4f0d-a66a-d616e5444243",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stratified train/test split\n",
    "train_test = tokenized_dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column=\"target\")\n",
    "train_dataset = train_test[\"train\"]\n",
    "test_dataset = train_test[\"test\"]\n",
    "\n",
    "# Verify sizes\n",
    "print(f\"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b4b95d3-258f-4d93-8ffa-3bca238209a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify split balance\n",
    "print(f\"\\nTrain size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n",
    "train_dist = pd.Series(train_dataset[\"target\"]).value_counts(normalize=True)\n",
    "test_dist = pd.Series(test_dataset[\"target\"]).value_counts(normalize=True)\n",
    "print(\"Train Label Distribution:\")\n",
    "print(train_dist)\n",
    "print(\"Test Label Distribution:\")\n",
    "print(test_dist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e8da2e-e3c0-43be-b71b-f1f683b20c21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify sequence lengths\n",
    "train_lengths = [len(sample['input_ids']) for sample in train_dataset]\n",
    "test_lengths = [len(sample['input_ids']) for sample in test_dataset]\n",
    "print(f\"\\nTrain lengths 512? {all(length == 512 for length in train_lengths)}\")\n",
    "print(f\"Test lengths 512? {all(length == 512 for length in test_lengths)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2b61d6-f384-4f0e-a82b-a228d4a3cca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32284d87-78ae-4164-a4aa-9e9daa548cb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into train and test set\n",
    "train, test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['target'])\n",
    "\n",
    "print(f\"New stratefied dataframe shapes: train is {train.shape}, train is {test.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9da8378-00ae-486e-b916-77f96b597aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train target counts:\")\n",
    "train.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d51a69c3-2086-4a8c-a285-8a7bd1ceb7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Test target counts:\")\n",
    "test.target.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e85ab23",
   "metadata": {},
   "source": [
    "## 3. Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09cef430-cf4a-4f61-aaf1-7f69d9336d43",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, Features, ClassLabel, Value\n",
    "import pandas as pd\n",
    "from transformers import DistilBertTokenizer, DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "path = \"c:\\\\Users\\\\Alex Chung\\\\Documents\\\\the_Lab\\\\Portfolio\\\\ml_engineering\\\\data\\\\sentiment140\\\\\"\n",
    "\n",
    "# Load and preprocess data\n",
    "df = pd.read_csv(path+\"training.1600000.processed.noemoticon.csv\", encoding=\"ISO-8859-1\", names=[\"target\", \"id\", \"date\", \"flag\", \"user\", \"text\"])\n",
    "df = df[[\"target\", \"text\"]].sample(10000, random_state=42)\n",
    "df[\"target\"] = df[\"target\"].map({0: 0, 4: 1})\n",
    "df = df.reset_index(drop=True)  # Avoid __index_level_0__\n",
    "\n",
    "# Define dataset features\n",
    "features = Features({\n",
    "    \"target\": ClassLabel(names=[\"negative\", \"positive\"]),\n",
    "    \"text\": Value(dtype=\"string\")\n",
    "})\n",
    "dataset = Dataset.from_pandas(df, features=features)\n",
    "\n",
    "# Check original distribution\n",
    "print(\"Original Label Distribution:\")\n",
    "print(df[\"target\"].value_counts(normalize=True))\n",
    "\n",
    "# Tokenize\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "\n",
    "# Stratified split\n",
    "train_test = tokenized_dataset.train_test_split(test_size=0.2, seed=42, stratify_by_column=\"target\")\n",
    "train_dataset = train_test[\"train\"]\n",
    "test_dataset = train_test[\"test\"]\n",
    "\n",
    "# Verify split\n",
    "print(f\"\\nTrain size: {len(train_dataset)}, Test size: {len(test_dataset)}\")\n",
    "train_dist = pd.Series(train_dataset[\"target\"]).value_counts(normalize=True)\n",
    "test_dist = pd.Series(test_dataset[\"target\"]).value_counts(normalize=True)\n",
    "print(\"Train Label Distribution:\")\n",
    "print(train_dist)\n",
    "print(\"Test Label Distribution:\")\n",
    "print(test_dist)\n",
    "\n",
    "# Inspect tokenized dataset\n",
    "print(\"\\nFirst Train Example:\")\n",
    "print(train_dataset[0])\n",
    "sample = train_dataset[0]\n",
    "decoded_text = tokenizer.decode(sample['input_ids'], skip_special_tokens=True)\n",
    "print(f\"Original: {sample['text']}\")\n",
    "print(f\"Decoded: {decoded_text}\")\n",
    "train_lengths = [len(sample['input_ids']) for sample in train_dataset]\n",
    "print(f\"Train lengths 512? {all(length == 512 for length in train_lengths)}\")\n",
    "\n",
    "# Set up Trainer\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=lambda eval_pred: {\n",
    "        \"accuracy\": (eval_pred.predictions.argmax(axis=1) == eval_pred.label_ids).mean()\n",
    "    }\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e42364-3a37-4351-a648-1474206ddb8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "print(\"Model loaded successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4b0b4bd-bba9-426a-b61c-e872c79042bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DistilBertForSequenceClassification, Trainer, TrainingArguments\n",
    "\n",
    "model = DistilBertForSequenceClassification.from_pretrained(\"distilbert-base-uncased\", num_labels=2)\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    ")\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3abcdd08-f9ad-45c9-acc9-10e1611b1e5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ed836c5-c8d8-4b86-936a-9e563e91b89c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1871add-eafa-4ef0-b435-189a72fa5fc8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c7382-6fc1-4c80-a238-8efb3fd39df1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "471103bd-ab6c-47d6-9b45-dd521b186403",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbe7ff39-10c1-4ee6-81f8-e0e279e20107",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b252e7-578a-4a58-aa5d-e6e271134037",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba88322-0258-427a-a6d8-492570b5d5b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "657525ae-fbe2-4304-8c7b-8fb7c8746507",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = pipeline(\"sentiment-analysis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f847c724-a32d-4549-8003-8d29d70bf9e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4686e517-df62-4be8-8a04-2a44e2000a82",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\"I've been waiting for a HuggingFace course my whole life.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e84d846-094d-431f-af13-debade9d5e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(df.text.to_list()[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4675c3a6",
   "metadata": {},
   "source": [
    "## 4. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24ff2779",
   "metadata": {},
   "source": [
    "## 1. Loading Data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fresh_venv_new (Python 3.10)",
   "language": "python",
   "name": "fresh_venv_new"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
